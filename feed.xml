<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://emalgorithm.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://emalgorithm.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-25T17:06:12+00:00</updated><id>https://emalgorithm.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Temporal Graph Learning in 2024 | Towards Data Science</title><link href="https://emalgorithm.github.io/blog/2024/temporal-graph-learning-in-2024-towards-data-science/" rel="alternate" type="text/html" title="Temporal Graph Learning in 2024 | Towards Data Science"/><published>2024-01-18T00:00:00+00:00</published><updated>2024-01-18T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2024/temporal-graph-learning-in-2024--towards-data-science</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2024/temporal-graph-learning-in-2024-towards-data-science/"><![CDATA[<p>Publish AI, ML &amp; data-science insights to a global community of data professionals. Continue the journey for evolving networks Many complex networks evolve over time including transaction networks, traffic networks, social networks and more. Temporal Graph Learning (TGL) is a fast growing field which aims to learn, predict and understand evolving networks. See our previous blog post for an introduction to temporal graph learning and a summary of advancements last year.In 2023, we saw significantly increased interest from both academia and the industry in the development of TGL. Compared to last year, the number of submissions at the temporal graph learning workshop @ NeurIPS 2023 tripled, resulting in 35 accepted papers. In addition, the temporal graph learning reading group, started in February 2023, has now hosted 28 research talks (find the recordings on YouTube). With nearly 200 researchers signed up for the reading group, we are glad to witness interest in the topic and an extremely active community.This post was co-authored with Emanuele Rossi, Michael Galkin ,Andrea Cini and Ingo Scholtes.This blog post covers a selection of exciting developments in TGL while pointing out research directions for 2024. We also ask leading researchers for their take on what the future holds for TGL. This blog also aims to provide references and act as starting points for those who want to learn more about temporal graph learning. Please share with us in the comment section any other advances you are excited about. For advancements on graph learning, checkout Michael Galkin‘s excellent blog post.Table of Contents:One of the driving forces of the rapid development of machine learning on graphs is the availability of standardized and diverse benchmarks such as the Open Graph Benchmark (OGB), the Long Range Graph Benchmark and GraphWorld. However, these benchmarks are designed for static graphs and lack the fine-grained timestamp information required for temporal graph learning. Therefore, progress in temporal graph learning has been held back by the lack of large high-quality datasets, as well as the lack of proper evaluation resulting in over-optimistic performances.To address this gap, the Temporal Graph Benchmark (TGB) was presented recently, including a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation for machine learning on temporal graphs. TGB provides a pypi package to automatically download and process nine datasets from five distinct domains with up to 72 million edges and 30 million timestamps. TGB also provides standardized evaluation motivated by real applications.TGB: Challenging and Realistic Benchmark for Temporal Graph Learning. Image source: Huang et al. 2023, by authors.TGB includes both link and node level tasks and an extensive empirical comparison of state-of-the-art TG models on all datasets. The first task is the dynamic link property prediction task which predicts the property (often existence) of a link between a pair of nodes at a future time. In TGB, this task is modeled as a ranking problem and evaluated with the filtered Mean Reciprocal Rank (MRR) metric. Results show that model rankings vary significantly across datasets with different ratios of test set edges which are never observed during training. In addition, model performance deteriorates as more negative samples (non-existence edges) are used in the evaluation. Interestingly, the extent of performance drop varies across models as well.In the dynamic node property prediction task, the goal is to predict the property of a node at a given time. More specifically we focus on the node affinity prediction task which models how the user preference towards different items shift over time. Here, we use the Normalized Discounted Cumulative Gain of the top 10 items (NDCG@10) to compare the relative order of the predicted items to that of the ground truth. Interesting, we found that single heuristics outperform existing TG models and this highlights the need for more models focusing on node level tasks in the future. The TGB leaderboard is public and you are welcome to submit your model via a google form. For more details, see the TGB blog post by the authors of this blog.”Link prediction in the realm of temporal graph learning poses a significant challenge. The learning algorithms must extend beyond the limited expressive power typically found in traditional message passing architectures like GNNs. Additionally, they must emphasize computational efficiency. A critical aspect of this is ensuring low latency in responding to link prediction queries, striking a balance between the expressive power of the model and the speed of its predictions in a dynamic and complex data environment.” – Pan Li, Assistant Professor, Georgia Institute of TechnologyA recent survey by Longa et al. provides a comprehensive overview of temporal GNNs. Many approaches proposed specialized architectures for dynamic link prediction, often aiming to capture important structure properties or correlations. For example, Luo et al. aimed to explicitly model the joint neighborhood of a set of nodes for future link prediction where they designed the Neighborhood-Aware Temporal network model (NAT). The joint neighborhood is not captured by traditional Graph Neural Network (GNN) based approaches as the node embedding vectors are generated independently for each node. In the following example, node v and w have the same structural contexts thus indistinguishable in the eyes of GNNs. In reality, the link between node u and v at t₃ is more likely to form due to the triadic closure law while this is not sure for the link between u and w at t₃. In comparison, NAT adapted a novel dictionary-type neighborhood representation which records k-hop neighborhood information and allows fast construction of structure features of joint neighborhood of multiple nodes. The dictionary representation is maintained by an efficient cache technique named N-cache. N-caches allowed NAT to construct the joint neighborhood features for a batch of node pairs for fast link prediction.GNN embeddings of node v and w would be identical. Image source: Luo et al. 2022Second, Yu et al. aim to capture long-term temporal dependencies by proposing DyGFormer, a new Transformer-based architecture for temporal graph learning. Given a query between node u and node v at time t, the first step is to extract historical first-hop interactions of node u and v before time t. This includes the encodings of neighbors, links, time intervals as well as the frequencies of every neighbor’s appearances of u and v. The assumption is that if u and v share more common historical neighbors in the past, then they are more likely to interact in the future. After encoding the historical interactions in a sequence, it is then divided into multiple patches and fed into a transformer for capturing temporal dependencies.Framework of DyGFormer Image source: Yu et al. 2023Another question is do we really need complicated model architectures for temporal networks? In a paper with the same name, <a href="https://openreview.net/forum?id=ayPPc0SyLv1">Cong et al.</a> examined the necessity of common modules used in temporal graph learning such as Recurrent Neural Network (RNN) and the self-attention mechanism. They showed that these modules are not always necessary for dynamic link prediction. In particular, their proposed GraphMixer model is based entirely on multi-layer perceptrons (MLPs) and neighbor mean-pooling while performing strongly against baselines with RNN and self-attention. GraphMixer contains three modules: a link-encoder summarizes the information from temporal links, a node-encoder extracts information from nodes and a link-classifier which combines the above information for prediction. Interestingly, Cong et al. argued that a trainable time-encoding function could cause instability during training and instead opted for a fixed time-encoding function z(t) = cos(tω) where fixed features capture the relative difference between two timestamps as shown below.Fixed time encoding function to convert t into a vector cos(tω). x-axis is the vector dimension and y-axis is the cosine value. Image source: Cong et al. 2023Lastly, Suresh et al. pointed out that existing methods maximizes accuracy independently over future links, ignoring the fact that future links often have dependency between each other. This is seen when a user selects among a list of items to purchase or a set of users to connect in a social network. Therefore, Suresh et al. treat dynamic link prediction as a ranking problem and propose Temporal Graph network for RANKing (TGRank) to learn to rank over a list of candidates. The pipeline of TGRank is shown below. The task query now contains a center node s (in the example) with a set of candidate nodes (all other nodes in the subgraph) and the goal is to rank the most likely candidate as the destination of node s. To this end, TGRank follows three steps. First, the node s is labeled differently from other nodes. Then, GNNs are used to diffuse the center node label to every ranking candidate. This parametrized label diffusion step aggregates timestamps, multiplicity as well as features of historical interactions along the network from the center node to all candidates and provides provably more expressive power for link prediction. Lastly, a list-wise loss is used to optimize the ranking amongst candidates jointly. Empirically, it is also shown that with a listwise ranking loss, popular models such as TGN and TGAT also perform better than their original setup with binary classification loss.Pipeline of TGRank. The center node is node s. Image source: Suresh et al. 2023”Basing our predictions primarily on the most related observations is sensible, yet not always straightforward, as relevant data relations often hide in plain sight. Unveiling them is a captivating challenge, particularly when they are dynamic or involve more than two entities.” – Daniele Zambon, PostDoc at The Swiss AI Lab IDSIAIn the temporal graph learning community, the term spatiotemporal graph has been often used to indicate a graph with fixed topology and node features that change over time, usually at discrete time steps corresponding to regularly sampled observations. More recently the problem of processing data with such a structure is being considered from a different perspective, i.e., by considering the dynamic node feature as time series and edges as functional dependencies among sequences of observations (Cini et al. 2023, Ming et al. 2023). From this perspective, which significantly deviates from many of the settings discussed elsewhere in this blog post, spatiotemporal graph-based representations allow for processing collections of correlated time series by taking advantage of the architectural biases typical of graph neural networks.Correlated time series with relational side information Image source: Cini et al. 2023, by authors.Such sets of correlated time series can be generated by sensors, either physical or not. An example is in the traffic domain, where time series might correspond to readings of sensors measuring the number of vehicles passing by at a crossroads. Each sensor will correspond to a different node and an adjacency matrix can be obtained, for instance, by joining with an edge only those sensors directly connected by a road segment. Besides traffic forecasting (Li et al. 2018, Yu et al. 2018), these representations have been used in a wide range of time series processing applications ranging from air quality monitoring (Chen et al. 2021) and energy analytics (Cini et al. 2023) to biomedical data processing (Zhang et al. 2022) and financial time series analysis (Matsunaga et al. 2019).Example of correlated time series from the traffic domain. Image source: tutorial at ECML PKDD 2023 by authors.To process this data, the standard message-passing framework needs to be updated to handle sequences of observations coming from the neighborhood of each node. This can easily be done by replacing the proper operators (i.e., the message and update functions) with operators able to process the data along the temporal dimension, e.g., recurrent cells (Seo et al. 2018), spatiotemporal convolutions (Wu et al. 2019) and attention-based architectures (Marisca et al. 2022). The resulting models are known as spatiotemporal graph neural networks (STGNNs) and there has been a large amount of research dedicated to coming up with effective architectures (see Ming et al. 2023). One of the main advantages of using STGNNs is that the same set of parameters can be used to forecast any subset of time series, while taking dependencies into account throughout the processing. This is a massive advantage over standard multivariate time series forecasting models that usually would have to forecast each time series separately or give up parameter sharing. Hybrid STGNNs, with some time-series-specific parameters, can also be considered and, as we have shown in a recent NeurIPS paper, often outperform models where all parameters are shared. Besides the model architecture, graph-based representations, as shown by Zambon et al., can also help in assessing the optimality of a forecasting model by focusing the spatiotemporal correlation analysis to interconnected nodes.A spatiotemporal graph neural network. Image by authors.Several challenges are inherent to the field, starting from dealing with irregularly sampled time series and missing data; indeed, both are quite common phenomena when dealing with actual cyber-physical systems. Luckily, graph-based models are useful in this context as well, for example allowing to condition the reconstruction on observations at neighboring sensors. Scalability is another major concern as differently from standard GNNs, message-passing is often performed w.r.t. each time step. Existing scalable architectures mostly rely on subsampling and/or pre-computed node features. When no prior relation information is available, the challenge becomes that of learning a latent graph directly from the time series. The problem has been tackled, for example, by directly learning an adjacency matric (e.g., Wu et al. 2019) or, under a probabilistic framework, by relying on reparametrization tricks and score-based estimators.A scalable spatiotemporal graph neural network Image source: Cini et al. 2023. by authorsSince this topic was not covered in last year’s blog post, the objective here was to give a short overview of the settings and the problems that can be modeled. Many directions are currently being explored, from graph state-space models and graph Kalman filters to diffusion-based and continuous space-time models. If you are interested in knowing more and/or using these models in practice, we recently released a comprehensive tutorial paper on the topic. You can also check out Torch Spatiotemporal (tsl), our library to build graph-based time series processing pipelines.There were surprisingly few temporal KG papers in this year’s top ML conferences: TILP (Xiong et al. 2023) on deriving temporal rule learning competitive with neural methods, and theory work by Chen and Wang to measure expressiveness of temporal GNNs. In fact, the most interesting (to me) papers on this topic were found at the TGL workshop at NeurIPS’23 (one more reason for you to follow the venue!), e.g., predicting future time intervals by Pop and Kostylev, or identifying leakages in standard benchmarking datasets by Pan et al. Finally, I’d outline the Unified Urban KG (UUKG) by Ning et al as a fresh look on temporal KG datasets that actually make practical sense and a use-case – modeling transportation flows in the city.UUKG illustrates the biggest problem of the shrinking temporal KG community – the lack of practically important tasks and datasets where it would be possible to demonstrate the utility of the data modeling paradigm in real-world tasks. That is, adding 1% of MRR/Hits@10 on 10-year old KG embedding benchmarks is rather useless these days compared to the successes of Geometric Deep Learning in biology or materials science (or compared to LLMs, but that’s a story for another day). Hopefully, we will see more UUKG-like practically useful datasets.Perhaps another adjacent area where temporal KGs might make a difference is heterogeneous graphs (that usually have typed edges) that are much more used in industry. For example, the recent RelBench (Relational Deep Learning Benchmark) formulates a temporal prediction problem over relational databases that can be easily converted to KGs or hypergraphs.”Einstein said the arrow of time flies in only one direction. […] And who among us, offered the chance, would not relive the day or hour in which we first knew love, or ecstasy, or made a choice that forever altered our future, negating a life we might have had? Such chances are rarely granted.” – Greg Iles, The Quiet GameOne reason why temporal graph learning is interesting is that – depending on the data at hand – it requires different perspectives. As an example, consider temporal graphs data with high-resolution (possibly continuous) time stamps. In such data, discrete-time graph learning techniques that utilize sequences of snapshot graphs require a coarse-graining of time, where each snapshot consists of edges occurring in a certain time interval. This coarse-graining allows to generalize (static) graph learning techniques to time series data. But it introduces a major issue: Each snapshots discards information on the temporal order in which edges occurred, which is the foundation of causal or time-respecting paths (Kempe et al. 2000). Like paths in static graphs, time-respecting paths are important since they tell us which nodes can causally influence each other indirectly. Below, we illustrate this in a simple temporal graph with two undirected edges (a,b) and (b,c) occurring at times t₁ and t₂ respectively. If (a,b) occurs before (b,c), a can causally influence c via a time-respecting path (indicated in purple) passing through b. If the temporal order of edges is reversed, a cannot causally influence c, since any influence must propagate backwards in time. Note that the directedness of the influence from a to c is due to the directed arrow of time and despite the fact that both edges are undirected. Moreover, while two edges (a,b) and (b,c) in a static, time-aggregated graph imply a transitive path from a via b to c (purple) and vice-versa (orange), this is not true for temporal graphs.Time-respecting path from node a to node c. Image by authors.Several works have shown that – due to the arrow of time – the causal topology of temporal graphs, i.e. which nodes can possibly causally influence each other via time-respecting paths, strongly differs from their static counterparts, with interesting implications for epidemic spreading (Pfitzner et al. 2013), diffusion speed (Scholtes et al. 2014), node centralities (Rosvall et al. 2014), or community detection (Lambiotte et al. 2019). Can we make deep learning methods aware of patterns in the causal topology of temporal graphs? Advances presented at this year show that this can be achieved based on models that generalize commonly used static representations of temporal graphs. Consider a weighted time-aggregated graph, where a (directed) edge (a,b) with weight five captures that (a,b) occurred five times in a temporal graph. Such a weighted, time-aggregated graph is illustrated in the bottom of panel 2 in the figure below.Pipeline to predict temporal centralities of nodes in a temporal graph. Image source: Heeg et al., by authorsEach edge in the temporal graph is a time-respecting path with length one. A weighted time-aggregated graph thus corresponds to a first-order model for the causal topology of a temporal graph, which captures time-respecting paths of length one. It neglects the temporal ordering of edges, since we only count how often each edge occurred. A line graph transformation enables us to generalize this idea to ** causality-aware models that facilitate temporal graph learning: We simply replace edges in the first-order graph by nodes in a <em>second-order graph, i.e. we turn edges (a,b) and (b,c) into nodes “a→b” and “b→c”, respectively. In the resulting second-order graph (see the top graph in panel 2 in figure), we can use edges to represent time-respecting paths of length two, i.e. edge (a→b, b→c</em>) indicates that a causally influence __ c via b. However, the reverse order of edges are not included. If the edges occur in reverse order, we do not includ_e (a→b, b→_c). Importantly, such a second-order graph is sensitive to the temporal ordering of edges, while a first-order graph is not! In Scholtes, 2017, this is generalized to higher orders, which yields k-th order De Bruijn graph models for the causal topology of temporal grap**hs.Qarkaxhija et al. have shown that neural message passing in such higher-order De Bruijn graphs yields a causality-aware graph neural network architecture for temporal graphs. Building on these De Bruijn Graph Neural Networks (DBGNN), in a poster at this year’s TGL workshop, Heeg and Scholtes address the challenge to predict temporal betweenness and closeness centralities of nodes. Since they are influenced by the arrow of time, temporal node centralities can drastically differ from static centralities. Moreover, it is costly to compute them! This is addressed by training a DBGNN model on a first time interval of a temporal graph, then using the trained model to forecast temporal centralities in the remaining data. The overall approach is illustrated above. Empirically results are promising and showcased the potential of causality-aware graph learning. We also hope to see more attention from the community in learning causal structure on temporal graphs in 2024.Interested in causality-aware temporal graph learning? Then there’s good news! The techniques above are implemented in the Open Source library pathpyG, which builds on PyG. There is an introductory video and a tutorial available. A recorded talk given in the temporal graph reading group provides an in-depth introduction of the underlying research.”Most important graph-structured data in real-world settings are temporal in nature. Explainable temporal graph models have the potential to unveil the long-standing questions on effective strategies and knowledge that can be leveraged to make temporal predictions, enabling extraction of insights from deep learning models and assisting scientific discovery and forecasting.” – Rex Ying, Assistant Professor, Yale University2023 saw the first approaches for explaining temporal GNN methods. Explainers are important for high-stake applications such as fraud detection and disease progression prediction in healthcare. Xia et al. proposed T-GNNExplainer as the first explainer designed for temporal graph models. T-GNNExplainer is model-agnostic and finds important events from a set of candidate events to best explain the model prediction. Xia et al. treat the problem of identifying a subset of explaining events as a combinatorial optimization problem by searching over a subset of the temporal graph within a given size. To tackle this, T-GNNExplainer employs an explorer-navigator framework. The navigator is trained from multiple target events to capture inductive correlations between events while the explorer searches out a specific combination of events based on Monte Carlo Tree Search, including node selection, node expansion, reward simulation and backprop. Which events are pruned is inferred from the navigator. The diagram below shows the framework of T-GNNExplainer.Framework of T-GNNExplainer. Image source: Xia et al. 2023Recently, Chen et al. argued that to form human intelligible explanations for temporal graph events requires the explanation to be events that are temporally proximate and spatially adjacent to that of the prediction, referred to as cohesive explanations. Utilizing temporal motifs, recurring substructures within the temporal graph, is a natural solution to form cohesive explanations for temporal graphs. This is because temporal motifs are crucial factors that guide the generative process of future events. In the following example, the preferential attachment rule (often facilitating influencer effect in e-commerce) and triadic closure rule (explains common-friend rules in social networks) forms cohesive and plausible explanations.Cohesive explanations are temporally approximate and spatially adjacent. Image source: Chen et al. 2023Therefore, Chen et al. proposed TempME, a novel Temporal Motif-based Explainer to identify important temporal motifs to explain temporal GNNs. The framework of TempME is shown in the below figure. First, temporal motifs surrounding the target prediction are extracted. Then these candidate motifs are encoded via the Motif Encoder which leverages event anonymization, message passing and graph pooling to generate an embedding for each motif. Then, based on the Information-bottleneck principle, TempME assigns importance scores to these motifs constrained by both explanation accuracy and information compression. Lastly, explanations are constructed by sampling from the Bernoulli distribution based on the importance score.Framework of TempME, numbers on the edges denote the temporal order. Image credit: Chen et al. 2023”As temporal graphs are adopted to be used for important tasks, like fraud detection, it is important to understand their failure points under adversarial attacks. Understanding and quantifying such blind spots is the first step towards creating robust and reliable temporal GNN models. Such efforts are necessary to ensure industry adoption of these models.” – Srijan Kumar, Assistant Professor at Georgia Institute of TechnologyAdversarial attacks can target the privacy of customers or affect critical decisions in financial systems. As temporal graph models are deployed to applications such as recommendation systems and fraud detection, it is important to investigate attacks and design defense mechanisms for TG models. Chen et al. proposed the first adversarial attack for dynamic link prediction called Time-aware Gradient Attack (TGA) for discrete time dynamic graphs. TGA rewires a limited number of links from the original network and the most valuable links to the predicted link are determined by the gradient information generated by the TG model.Overview of the Temporal Dynamics-aware Perturbation attack. The attacker can flip the prediction of the model while evading detection. Image source: Sharma et al. 2023Recently, Sharma et al. argued that effective attacks on temporal graphs must optimize both edge and time perturbations while preserving the original graph evolution. This is because drastic attacks that disturb the graph evolution would be easily detected by anomaly detection methods. Therefore, Sharma et al. formulated evolution-preserving attacks on discrete-time dynamic graphs as the Temporal Dynamics-Aware Perturbation (TDAP) constraint. TDAP asserts that perturbations added at a given timestamp must only be a small fraction of the actual number of changes with respect to the preceding timestamp. TDAP is shown to preserve the rate of change in both the structure and the embedding spaces. An overview of TDAP is shown in the figure below. Sharma et al. then proposes a novel attack method called Temporal Dynamics-aware Projected Gradient Descent (TD-PGD) which is shown to have a closed-form projection operator under the TDAP constraint. An online version of TD-PGD is also proposed where perturbations can be added in real time. Lastly, it is shown empirically that TDAP-constrained perturbations can indeed evade attacks by embedding-based anomaly detection methods.In 2023 saw a significant push towards standardized libraries and benchmarks for temporal graph learning. TGB provides an open and standardized benchmark for node and link level tasks. DyGLib is a library which includes standard training pipelines, extensible coding interfaces, and comprehensive evaluation strategies for temporal graph learning. Zhang et al. introduced the novel concept of Live Graph Lab, providing live graphs according to blockchain transactions. With a set of tools for downloading, parsing, cleaning, and analyzing blockchain transactions, Live Graph Lab offers researchers the opportunity to extract update to date temporal graph data any time and use it for analysis or testing. Zhou et al. noticed that node memory used in TG models favors small batch sizes and needs to be maintained synchronously across all trainers. Therefore, they proposed DistTGL, an efficient and scalable solution to train memory-based TGNNs on distributed GPU clusters. Enabling multi-GPU training on large datasets is an important direction to deploy TG models on large datasets. We present an updated list of libraries and benchmarks for temporal graph learning:To join the fast growing TG community, sign up for the weekly temporal graph reading group <a href="https://forms.gle/4UuiTUDEqkvQ4pHC8">here</a>. Visit the reading group website and Youtube to see all upcoming and recorded past talks. We also include the invitation link to the TG slack on the website (updated monthly). The second edition of temporal graph learning workshop @ NeurIPS 2023 includes an exciting lineup of 35 posters for cutting edge research in TG. You can also find the talk recordings on the NeurIPS virtual site (will be public in a month). If you want to be a reviewer for the next edition of the workshop, sign up here. To find more about the research from the authors of this post, see our websites: Shenyang(Andy) Huang, Emanuele Rossi, Michael Galkin ,Andrea Cini and Ingo Scholtes. Hope to see you at the reading group or the next edition of the workshop!Logo of the NeurIPS 2023 Temporal Graph Learning Workshop. Image by authors.Written ByShare This ArticleTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.Step-by-step code guide to building a Convolutional Neural Network A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Feature engineering, structuring unstructured data, and lead scoring An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Continue the journey for evolving networks]]></summary></entry><entry><title type="html">Temporal Graph Benchmark | Towards Data Science</title><link href="https://emalgorithm.github.io/blog/2023/temporal-graph-benchmark-towards-data-science/" rel="alternate" type="text/html" title="Temporal Graph Benchmark | Towards Data Science"/><published>2023-12-09T00:00:00+00:00</published><updated>2023-12-09T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2023/temporal-graph-benchmark--towards-data-science</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2023/temporal-graph-benchmark-towards-data-science/"><![CDATA[<p>Publish AI, ML &amp; data-science insights to a global community of data professionals. Challenging and realistic datasets for temporal graph learning In recent years, significant advances have been made in machine learning on static graphs, accelerated by the availability of public datasets and standardized evaluation protocols, such as the widely adopted Open Graph Benchmark (OGB). However, many real-world systems such as social networks, transportation networks, and financial transaction networks evolve over time with nodes and edges constantly added or deleted. They are often modeled as temporal graphs. Until now, progress in temporal graph learning has been held back by the lack of large high-quality datasets, as well as the lack of proper evaluation thus leading to over-optimistic performance.To address this, we present Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation for machine learning on temporal graphs. Inspired by the success of OGB, TGB automates dataset downloading and processing as well as evaluation protocols, and allows users to compare model performance using a leaderboard. We hope TGB would become a standardized benchmark for the temporal graph community and facilitate the development of novel methods and improve the understanding of large temporal networks.This post is based on our paper Temporal Graph Benchmark for Machine Learning on Temporal Graphs (NeurIPS 2023 Datasets and Benchmarks Track) and was co-authored with Emanuele Rossi. Find more temporal graph work from my website. Want to learn more about temporal graphs? Join the Temporal Graph Reading Group and Temporal Graph Learning Workshop @ NeurIPS 2023 to learn more about state-of-the-art TG research.In the last few years, the field of machine learning for static graphs has experienced a significant boost, largely due to the advent of public datasets and established benchmarks like the Open Graph Benchmark (OGB), the Long Range Graph Benchmark, and the TDC Benchmark. However, many real-world systems such as social networks, transportation networks, and financial transaction networks are temporal: they evolve over time. Until now, the advancement in temporal graphs has been significantly hampered by the lack of large, high-quality datasets and comprehensive evaluation frameworks. This scarcity, coupled with evaluation limitations, has resulted in almost perfect AP or AUROC scores across models on popular datasets such as Wikipedia and Reddit, leading to an over-optimistic assessment of model performance and a challenge in differentiating between competing models.Lack of datasets. Common TG datasets only contain a few million edges, significantly smaller than the scale seen in real temporal networks. Furthermore, these datasets are mostly restricted to the domain of social and interaction networks. As network properties often vary significantly across domains, it is important to benchmark on a variety of domains as well. Lastly, there is a lack of datasets for node-level tasks, causing most of the methods to only focus on link prediction. To solve this challenge, TGB contains nine datasets from five distinct domains which are orders of magnitude larger in terms of the number of nodes, edges and timestamps. In addition, TGB proposes four datasets for the novel node affinity prediction task.Simplistic Evaluation. Dynamic link prediction was commonly framed as a binary classification task: positive (true) edges have a label of one, while negative (non-existing) edges have a label of zero. When evaluating, one negative edge per positive one was sampled by keeping the source node fixed and choosing the destination node uniformly at random. This evaluation only considers a small amount of easy to predict negative edges, leading to inflated model performance with many models obtaining &gt;95% AP on Wikipedia and Reddit (Poursafaei et al. 2022, Rossi et al. 2020, Wang et al. 2021, Souza et al. 2022). In TGB we treat the link prediction task as a ranking problem and make the evaluation more robust. We show that the improved evaluation results in more realistic performance and highlights clear gaps between different models.In TGB, we focus on continuous time temporal graphs, as defined by Kazemi et al. 2020. In this setting, we denote temporal graphs as timestamped edge streams consisting of triplets of (source, destination, , timestamp). Note that temporal edges can be weighted, directed, while both nodes and edges can optionally have features.Additionally, we consider the streaming setting, where a model can incorporate new information at inference time. In particular, when predicting a test edge at time t, the model can access[1] all edges occurred before t, including test edges. However, back-propagation and weight updates with the test information are not permitted.TGB contains nine datasets, seven of which are curated for this work while two are from previous literature. The datasets are temporally split into training, validation and test sets with ratio of 70/15/15. Datasets are categorized based on their number of edges: small (&lt;5 million), medium (5–25 million) and large (&gt; 25 million).TGB datasets also have distinct domains and time granularities (from UNIX timestamp to annually). Lastly, dataset statistics are highly diverse too. For example, the Surprise Index, defined by the ratio of test edges never observed in the training set, varies significantly across datasets. Many TGB datasets also contain many novel nodes in the test set which requires inductive reasoning.TGB datasets are also tied to real world tasks. For example, tgbl-flight dataset is a crowd sourced international flight network from 2019 to 2022 where the airports are modeled as nodes while edges are flights between airports for a given day. The task is to predict whether a flight will happen between two specific airports on a future date. This is useful for forecasting potential flight disruptions such as cancellation and delays. For instance, during the COVID-19 pandemic, many flight routes were canceled to combat the spread of COVID-19. The prediction of the global flight network is also important for studying and forecasting the spread of disease such as COVID-19 to new regions as seen in Ding et al. 2021. Detailed dataset and task descriptions are provided in Section 4 of the paper.The goal of dynamic link property prediction is to predict the property (often the existence) of a link between a node pair at a future timestamp.Negative Edge Sampling. In real applications, the true edges are not known in advance. Therefore, a large number of node pairs are queried, and onlypairs with the highest scores are treated as edges. Motivated by this, we frame the link prediction task as a ranking problem and sample multiple negative edges per each positive edge. In particular, for a given positive edge (s,d,t), we fix the source node s and timestamp t and sample q different destination nodes d. For each dataset, q is selected based on the trade-off between evaluation completeness and test set inference time. Out of the q negative samples, half are sampled uniformly at random, while the other half are historic negative edges (edges that were observed in the training set but are not present at time t).Performance metric. We use the filtered Mean Reciprocal Rank (MRR) as the metric for this task, as it is designed for ranking problems. The MRR computes the reciprocal rank of the true destination node among the negative or fake destinations and is commonly used in recommendation systems and knowledge graph literature.Results on small datasets. On the small tgbl-wiki and tgbl-reviewdatasets, we observe that the best performing models are quite different. In addition, the top performing models on tgbl-wiki such as CAWN and NAT have a significant reduction in performance on tgbl-review. One possible explanation is that the tgbl-reviewdataset has a much higher surprise index when compared to the tgbl-wikidataset. The high surprise index shows that a high ratio of test set edges is never observed in the training set thus tgbl-reviewrequires more inductive reasoning. In tgbl-review, GraphMixer and TGAT are the best performing models. Due to their smaller size, we are able to sample all possible negatives for tgbl-wikiand one hundred negatives for tgbl-reviewper positive edge.Most methods run out of GPU memory for these datasets thus we compare TGN, DyRep and Edgebank on these datasets due to their lower GPU memory requirement. Note that some datasets such as tgbl-commentor tgbl-flightspanning multiple years thus potentially resulting in distribution shift over its long time span.Insights. As seen above in tgbl-wiki, the number of negative samples used for evaluation can significantly impact model performance: we see a significant performance drop across most methods, when the number of negative samples increases from 20 to all possible destinations. This verifies that indeed, more negative samples are required for robust evaluation. Curiously, methods such as CAWN and Edgebank have relatively minor drop in performance and we leave it as future work to investigate why certain methods are less impacted.Next, we observe up to two orders of magnitude difference in training and validation time of TG methods, with the heuristic baseline Edgebank always being the fastest (as it is implemented simply as a hashtable). This shows that improving the model efficiency and scalability is an important future direction such that novel and existing models can be tested on large datasets provided in TGB.The goal of dynamic node property prediction is to predict the property of a node at any given timestamp t. As there is a lack of large public TG datasets with dynamic node labels, we introduce the node affinity prediction task to investigate node level tasks on temporal graphs. If you would like to contribute a novel dataset with node labels, please reach out to us.Node affinity prediction. This task considers the affinity of a subset of nodes (e.g., users) towards other nodes (e.g. items) as its property, and how the affinity naturally changes over time. This task is relevant for example in recommendation systems, where it is important to provide personalized recommendations for a user by modeling their preference towards different items over time. Here, we use the Normalized Discounted Cumulative Gain of the top 10 items (NDCG@10) to compare the relative order of the predicted items to that of the ground truth. The label is generated by counting the frequency of user interaction with different items over a future period.Results. For this task, we compare TG models with two simple heuristics: persistence forecast, predicting the most recent observed node label for the current time and moving average, the average over node labels in the past few steps. The key observation here is that on this task, simple heuristics such as persistence forecast and moving average are strong contenders to TG methods and in most cases, outperforming them. This highlights the need to develop more TG methods for node-level tasks in the future.How to use TGB? The above shows the ML pipeline in TGB. You can automatically download datasets and processes them into numpy, PyTorchand PyGcompatible data formats. Users only need to design their own TG models which can be easily tested via TGB evaluators to standardize evaluation. Lastly, the public and online TGB leaderboards help researchers track recent progress in the temporal graph domain. You can install the package easily:Finally, you can submit your model performance to the TGB leaderboard. We ask that you provide link to your code and a paper describing your approach for reproducibility. To submit, please fill in the google form.To enable realistic, reproducible, and robust evaluation for machine learning on temporal graphs, we present the Temporal Graph Benchmark, a collection of challenging and diverse datasets. With TGB datasets and evaluation, we found that model performance varies significantly across datasets, thus demonstrating the necessity to evaluate on a diverse range of temporal graph domains. In addition, on the node affinity prediction task, simple heuristics outperform TG methods thus motivating the development of more node-level TG models in the future.Integration into PyG. Matthias Fey (Kumo.AI), Core lead of PyG, announced at the Stanford Graph Learning Workshop 2023 that TGB will be integrated into future versions of PyG. Stay tuned!TGX library. We are currently developing an utility and visualization Python library for temporal graphs, named TGX. TGX supports 20 built in temporal graph datasets from TGB and Poursafaei et al. 2022.Community Feedback and Dataset Contributions. TGB is a community driven project and we would like to thank all the community members who provided suggestions to us via email or Github issues. If you have any suggestions or want to contribute new datasets to TGB, please reach out to us via email or create an issue on Github. We are looking for large scale datasets, especially those for dynamic node or graph classification tasks.Written ByShare This ArticleTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.Step-by-step code guide to building a Convolutional Neural Network A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Feature engineering, structuring unstructured data, and lead scoring An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Columns on TDS are carefully curated collections of posts on a particular idea or category… Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Challenging and realistic datasets for temporal graph learning]]></summary></entry><entry><title type="html">GNNs on Directed Graphs</title><link href="https://emalgorithm.github.io/blog/2023/dirgnn/" rel="alternate" type="text/html" title="GNNs on Directed Graphs"/><published>2023-06-08T00:00:00+00:00</published><updated>2023-06-08T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2023/dirgnn</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2023/dirgnn/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/cover-480.webp 480w,/assets/img/blog/dir_gnn/cover-800.webp 800w,/assets/img/blog/dir_gnn/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/cover.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Illustration based on Shutterstock.</figcaption> </figure> <p><em>This post is based on the paper <a href="https://arxiv.org/abs/2305.10498">Edge Directionality Improves Learning on Heterophilic Graphs</a><d-cite key="dirgnn_rossi_2023"></d-cite>, a collaboration with <a href="https://twitter.com/Bertrand_Charp">Bertrand Charpentier</a>, <a href="https://twitter.com/Francesco_dgv">Francesco Di Giovanni</a>, <a href="https://twitter.com/ffabffrasca">Fabrizio Frasca</a> and <a href="https://twitter.com/guennemann">Stephan Günnemann</a></em><d-footnote>The title of this post, "Direction improves graph learning," is an intentional play on a previous work J. Gasteiger, S. Weissenberger, and S. Günnemann, <a href="https://proceedings.neurips.cc/paper_files/paper/2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf" target="_blank">Diffusion improves graph learning,</a> (2019), NeurIPS by one of the authors, which showed that a diffusion-based graph rewiring scheme (DIGL) improves the performance of GNNs in homophilic settings. Here, we focus on the heterophilic case.</d-footnote>. <em>The code for the paper can be found <a href="https://github.com/emalgorithm/directed-graph-neural-network">here</a>. The same blog post has been also published on <a href="https://medium.com/data-science/direction-improves-graph-learning-170e797e94fe">Medium</a>.</em></p> <h2 id="introduction">Introduction</h2> <p>Many interesting real-world graphs, encountered in modelling social, transportation, financial transactions, or academic citation networks, are <em>directed</em>. The direction of the edges often conveys crucial insights, otherwise lost if one considers only the connectivity pattern of the graph.</p> <p>In contrast, most Graph Neural Networks (GNNs) that have made remarkable strides in a variety of graph ML applications, operate under the assumption that the input graph is <em>undirected</em>. Making the input graph undirected has become so prevalent over the years that PyTorch-Geometric, one of the most popular GNN libraries, includes a general utility function that automatically makes graphs undirected when loading datasets<d-footnote><a href="https://github.com/pyg-team/pytorch_geometric/blob/66b17806b1f4a2008e8be766064d9ef9a883ff03/torch_geometric/io/npz.py#L26">This Pytorch-Geometric routine</a> is used to load datasets stored in an npz format. It makes some directed datasets, such as <a href="https://github.com/pyg-team/pytorch_geometric/blob/6fa2ee7bfef32311df73ca78266c18c4449a7382/torch_geometric/datasets/citation_full.py#L99">Cora-ML</a> and <a href="https://github.com/pyg-team/pytorch_geometric/blob/6fa2ee7bfef32311df73ca78266c18c4449a7382/torch_geometric/datasets/citation_full.py#L99">Citeseer-Full</a>, automatically undirected without any option to get the directed version instead. </d-footnote>.</p> <p>This inclination towards undirected graphs comes, in our opinion, from two “primordial sins” of GNNs. First, undirected graphs have symmetric Laplacians with orthogonal eigenvectors offering a natural generalisation of the Fourier transform, on which early spectral GNNs relied to function properly. Second, early datasets used to benchmark GNNs were predominantly <em>homophilic</em> graphs<d-footnote>Homophily refers to the assumption that nodes with similar properties (typically labels and sometimes features) tend to be connected. In homophilic graphs, the neighbourhood of a node looks like the node itself, often allowing to predict the property of a node from a simple aggregation (e.g., averaging) of the neighbours. Graphs violating this assumption are called heterophilic.</d-footnote>, such as Cora and Pubmed<d-footnote>The Cora dataset was introduced by <a href="https://people.cs.umass.edu/~mccallum/data.html">Andrew McCallum</a> in the late 1990s and is for GNNs what MNIST Digits datasets is for CNNs. </d-footnote>. On such datasets disregarding the direction by converting the directed graph into an undirected one appears to be advantageous, early evidence whereof has helped cement the “undirected” paradigm.</p> <p>We challenge this <em>status quo</em> in our recent paper<d-cite key="dirgnn_rossi_2023"></d-cite>, showing that directionality can bring extensive gains in heterophilic settings.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/aggregated_results-480.webp 480w,/assets/img/blog/dir_gnn/aggregated_results-800.webp 800w,/assets/img/blog/dir_gnn/aggregated_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/aggregated_results.png" class="img-fluid" width="500px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Direction is largely useless in homophilic graphs (left), an observation that has led to the majority of current GNNs disregarding this information. In contrast, in the heterophilic setting (right), the use of direction can bring large gains (10% to 15%) if used correctly, like proposed in our Dir-GNN framework.</figcaption> </figure> <h2 id="measuring-homophily-in-directed-graphs">Measuring Homophily in Directed Graphs</h2> <p>The homophily of a graph is typically measured as the fraction of neighbours with the same label as the node itself, averaged across all nodes (<em>node homophily</em>). For directed graphs, we propose the <em>weighted node homophily</em>:</p> \[h(\mathbf{S}) = \frac{1}{|V|} \sum_{i \in V} \frac{\sum\limits_{j \in \mathcal{N}(i)} s_{ij} I[y_i = y_j]}{\sum\limits_{j \in \mathcal{N}(i)} s_{ij}}\] <p>where \(I\) denotes the indicator function, \(n\) is the number of nodes, and \(\mathbf{S}\) is a general adjacency matrix, which can be picked up as \(\mathbf{A}\) or \(\mathbf{A}^\top\), or as higher-order matrices, such as \(\mathbf{AA}^\top\) or \(\mathbf{A}^2\) for directed graphs, or as the symmetric matrix \(\mathbf{A}_u = \frac{\mathbf{A} + \mathbf{A}^\top}{2}\) and its higher-order counterpart \(\mathbf{A}_u^2\), if the graph is considered as undirected.</p> <p>Even when 1-hop neighbours are heterophilic, the situation may change when going to farther nodes. Compared to the undirected case, there are four distinct 2-hops in directed graphs represented by the matrices \(\mathbf{A}^2\), \((\mathbf{A}^\top)^2\), \(\mathbf{AA}^\top\) and \(\mathbf{A}^\top\mathbf{A}\), which can manifest different levels of (weighted) homophily.</p> <p>Given that GNNs operate through multiple-hop aggregations, they can leverage the homophily of any 2-hop (or even further hops) of a graph. To have a comprehensive metric capturing the maximum homophily a GNN can leverage in principle, we introduce the notion of <em>effective homophily</em>, defined as the maximum weighted node homophily at any hop of the graph.</p> <p>Empirically, we observe that the effective homophily of directed homophilic datasets is left unchanged when making the graph undirected. In heterophilic graphs, in contrast, this conversion decreases the effective homophily by almost 30% on average.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/homophily_table-480.webp 480w,/assets/img/blog/dir_gnn/homophily_table-800.webp 800w,/assets/img/blog/dir_gnn/homophily_table-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/homophily_table.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">We compare the weighted homophily of both directed and undirected diffusion matrices for a variety of homophilic and heterophilic datasets. We mark in bold the highest entry for each row. The effective homophily of the directed graph (h_d^{(\text{eff})}) is much larger than that of the undirected graph (h_u^{(\text{eff})}) for heterophilic datasets, suggesting a potential gain from using directionality effectively.</figcaption> </figure> <h2 id="a-toy-example">A Toy Example</h2> <p>In particular, we observe that \(\mathbf{AA}^\top\) and \(\mathbf{A}^\top\mathbf{A}\) consistently appear to be the “most homophilic matrices” for heterophilic graphs.</p> <p>To provide an intuition about why this is the case, imagine we are trying to predict the publication year of a specific academic paper, for instance, the Kipf &amp; Welling 2016 GCN paper, given the directed citation network and the year of publication of the other papers. Consider two different kinds of 2-hop relationships: one where we look at papers cited by the papers that our paper of interest <em>v</em> cites (represented by the \(v^{th}\) row of the matrix \(\mathbf{A}^2\)), and another where we look at papers that cite the same sources as our paper (represented by \((\mathbf{AA}^\top)_v\)).</p> <p>In the first case (\(\mathbf{A}^2\)), let us start from the GCN paper and follow its citations twice. We land on a paper by Frasconi <em>et al.</em> from 1998. This older paper does not give us much helpful information about when our GCN paper was published because it is too far in the past.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/diagram.svg" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/diagram.svg" class="img-fluid" width="400px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Simple example of a directed citation network with publication year as the node labels.</figcaption> </figure> <p>Simple example of a directed citation network with publication year as the node labels.</p> <p>In the second case (\(\mathbf{AA}^\top\)), we begin with the GCN paper, follow a citation, and then come back to a paper that cites the same source, like the 2017 GAT paper. This paper is much closer to our GCN paper’s publication year and thus provides a better clue. More generally, nodes that share more references, like in our second example, will have higher scores in \(\mathbf{AA}^\top\), and thus contribute more to our final prediction.</p> <p>Now, consider an undirected 2-hop relationship (\(\mathbf{A}_u^2\)), which is just the average of the four possible 2-hop matrices. This includes our first type (like Frasconi et al.), which was not very helpful. Therefore, the highly useful \(\mathbf{AA}^\top\) matrix gets diluted by less informative matrices, like \(\mathbf{A}^2\), leading to a less homophilic operator, resulting in a less reliable predictor overall.</p> <p>While we have used a citation network in our example, this intuition applies more broadly. In a social network, for instance, an influencer’s characteristics are more likely to resemble those of users who share many followers with them, represented by \(\mathbf{A}^\top\mathbf{A}\). Similarly, in a transaction network, two accounts sending money to the same set of accounts (captured by \(\mathbf{AA}^\top\)), are likely to exhibit similar behaviour.</p> <h2 id="dir-gnn-directed-graph-neural-network">Dir-GNN: Directed Graph Neural Network</h2> <p>In order to leverage directionality effectively, we propose the <em>Directed Graph Neural Network</em> (Dir-GNN) framework, which extends MPNNs to directed graphs by performing separate aggregations over the in- and out-neighbours of a node:</p> \[\begin{align} \mathbf{m}^{(k)}_{i,\leftarrow} &amp;= \mathrm{AGG}^{(k)}_{\leftarrow}\left(\{\{ \mathbf{x}_{j}^{(k-1)} : \, (j,i)\in E\}\} \right) \notag \\ \mathbf{m}^{(k)}_{i,\rightarrow} &amp;= \mathrm{AGG}^{(k)}_{\rightarrow}\left(\{\{ \mathbf{x}_{j}^{(k-1)} : \, (i,j)\in E \}\} \right) \notag \\ \mathbf{x}_{i}^{(k)} &amp;= \mathrm{COM}^{(k)}\left(\mathbf{x}_{i}^{(k-1)},\mathbf{m}^{(k)}_{i,\leftarrow}, \mathbf{m}^{(k)}_{i,\rightarrow}\right) \notag \end{align}\] <p>where the aggregation maps \(\mathrm{AGG}^{(k)}_{\leftarrow}\) and \(\mathrm{AGG}^{(k)}_{\rightarrow}\), as well as the combination maps \(\mathrm{COM}\) are learnable (usually a small neural network). Importantly, \(\mathrm{AGG}^{(k)}_{\leftarrow}\) and \(\mathrm{AGG}^{(k)}_{\rightarrow}\) can have independent set of parameters to allow for different aggregations over in- and out-edges<d-footnote>It's important to note that we are not the first to deal with directed graphs and to propose separate aggregation of in- and out-neighbours. However, our contribution is providing a more comprehensive treatment of directed graphs, which includes 1) a general framework (Dir-GNN), 2) overarching empirical evidence for the benefit of directionality, particularly in the context of heterophily, and 3) a starting point to analyse the expressiveness of models for directed graphs. See the "Related Work" section of <a href="https://arxiv.org/abs/2305.10498">our paper</a> for a more thorough overview of previous works.</d-footnote>.</p> <p>Interestingly, this procedural pattern resembles the one implemented by a natural extension of the 1-WL algorithm to directed graphs<d-footnote>While several extensions of the WL test on directed graphs have been proposed, the variant discussed by <a href="https://www.lics.rwth-aachen.de/global/show_document.asp?id=aaaaaaaaabbtcqu">Color Refinement and its Applications</a><d-cite key="Grohe2021ColorRA"></d-cite>, treats in- and out-neighbours separately. </d-footnote>. This connection is instrumental: in terms of discriminative power, we show that Dir-GNN is strictly more powerful than standard MPNNs, which either convert the graph to undirected or propagate messages only in the direction of the edges.</p> <p>Our framework is also flexible: it is easy to define directed counterparts of specific architectures such as GCN, GraphSAGE or GAT. For example, we can define Dir-GCN as:</p> \[\mathbf{X}^{(k)} = \sigma\left(\mathbf{A}_{\rightarrow}\mathbf{X}^{(k-1)}\mathbf{W}^{(k)}_{\rightarrow} + \mathbf{A}^\top_{\rightarrow}\mathbf{X}^{(k-1)}\mathbf{W}^{(k)}_{\leftarrow}\right)\] <p>where \(\mathbf{A}_{\rightarrow} = \mathbf{D}_{\rightarrow}^{-1/2}\mathbf{A}\mathbf{D}_{\leftarrow}^{-1/2}\) and \(\mathbf{D}_{\leftarrow}\) and \(\mathbf{D}_{\rightarrow}\) represent the diagonal in- and out-degree matrices, respectively.</p> <p>We also show that Dir-GNN, when iteratively applied over multiple layers, leads to more homophilic aggregations. Unlike other models, Dir-GNN can access the four 2-hop matrices \(\mathbf{A}^2\), \((\mathbf{A}^\top)^2\), \(\mathbf{AA}^\top\) and \(\mathbf{A}^\top\mathbf{A}\) and learn to weight them differently. In contrast, a model operating on the undirected graph has only access to \(\mathbf{A}_u^2\), while models propagating information exclusively along in- or out-edges are limited to \((\mathbf{A}^\top)^2\) and \(\mathbf{A}^2\) respectively.</p> <p>Dir-GNN, thanks to its separate aggregation of the two directions, is therefore the only model operating on \(\mathbf{AA}^\top\) and \(\mathbf{A}^\top\mathbf{A}\), which we have shown to be the most homophilic matrices and therefore the most reliable predictor.</p> <h2 id="experimental-results">Experimental Results</h2> <p>We first compared GraphSAGE and its directed extension (Dir-SAGE) on a synthetic task requiring directionality information. The results confirm that only Dir-SAGE(in+out), with access to both in- and out-edges is able to almost perfectly solve the task. The model acting on the undirected version of the graph performs no better than chance, while the models acting on only in- or out-edges perform similarly obtaining around 75% accuracy.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/synthetic_accuracy-480.webp 480w,/assets/img/blog/dir_gnn/synthetic_accuracy-800.webp 800w,/assets/img/blog/dir_gnn/synthetic_accuracy-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/synthetic_accuracy.png" class="img-fluid" width="350px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">When examining the performance of GraphSAGE and its Dir- extensions on a synthetic task requiring directionality information, only Dir-SAGE(in+out), which utilises information from both directions, is capable of solving the task.</figcaption> </figure> <p>We further validated our approach with an ablation study comparing GCN, GraphSAGE and GAT base models with their Dir- extensions.On heterophilic datasets, using directionality brings exceptionally large gains (10% to 20% absolute) in accuracy across all three base GNN models. Moreover, Dir-GNN beats state-of-the-art models designed especially for heterophilic graphs. These results suggest that, when present, using the edge direction can significantly improve learning on heterophilic graphs. In contrast, discarding it is so harmful that not even complex architectures can make up for this loss of information.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/dir_gnn/full_results-480.webp 480w,/assets/img/blog/dir_gnn/full_results-800.webp 800w,/assets/img/blog/dir_gnn/full_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/dir_gnn/full_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">New state-of-the-art results on heterophilic graphs, by using direction wisely.</figcaption> </figure> <p>On the other hand, on homophilic datasets using directionality leaves the performance unchanged (or even slightly hurts). This is in line with our findings that using directionality as in our framework generally increases the effective homophily of heterophilic datasets, while leaving it almost unchanged for homophilic datasets.</p> <p>In conclusion, our paper showcases the benefit of leveraging directionality in GNNs, particularly in the case of heterophilic graphs. We hope that these findings will instigate a paradigm shift, elevating direction as a first-class citizen in GNNs. In short, <strong>think twice before making your graph undirected!</strong></p> <p><em>We would like to thank Christopher Morris and Chaitanya K. Joshi for insightful discussions and pointing us to relevant papers.</em></p>]]></content><author><name>Emanuele Rossi</name></author><summary type="html"><![CDATA[Graph Neural Networks (GNNs) are extremely effective at modelling relational data. However, current GNN models frequently assume the input graph to be undirected, overlooking the inherent directionality of many real-world graphs, such as social, transportation, transaction, and citation networks. In this blog post, we explore the impact of edge directionality in the context of heterophilic graphs and outline Dir-GNN, a novel message passing scheme for directed graphs allowing a separate aggregation of incoming and outgoing edges. Despite its simplicity, this scheme achieves significant performance improvement on multiple real-world heterophilic directed graphs.]]></summary></entry><entry><title type="html">Learning Network Games</title><link href="https://emalgorithm.github.io/blog/2023/network_games/" rel="alternate" type="text/html" title="Learning Network Games"/><published>2023-04-20T00:00:00+00:00</published><updated>2023-04-20T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2023/network_games</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2023/network_games/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/cover-480.webp 480w,/assets/img/blog/network_games/cover-800.webp 800w,/assets/img/blog/network_games/cover-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/cover.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Illustration based on Shutterstock.</figcaption> </figure> <p><em>This post is based on the paper <a href="https://arxiv.org/abs/2206.08119">Learning to infer the structure of network games</a><d-cite key="rossi2022networkgames"></d-cite>, a collaboration with <a href="https://scholar.google.it/citations?user=NUdNFucAAAAJ&amp;hl=it">Federico Monti</a>, <a href="https://yleng.github.io/www/">Yan Leng</a>, and <a href="https://eng.ox.ac.uk/people/xiaowen-dong/">Xiaowen Dong</a>. The same blog post has been also published on <a href="https://medium.com/data-science/learning-network-games-29970aee44bb">Medium</a>.</em></p> <h2 id="network-games">Network Games</h2> <p><a href="https://en.wikipedia.org/wiki/Game_theory">Game theory</a> is a mathematical framework for modelling and analysing situations where multiple decision makers interact with each other, and where the outcome of each decision depends on the actions of all players involved. In <em>network games</em><d-footnote>See <a href="https://web.stanford.edu/~jacksonm/GamesNetworks.pdf">Games on Networks</a><d-cite key="jackson2014games"></d-cite> for an overview.</d-footnote> the players are connected in a network (graph), and the outcome of the game depends not only on the players’ strategies but also on the structure of the network. Each player tries to maximise their <em>utility function</em>, which in the case of network games depends both on their own actions and the actions of their neighbours.</p> <p><em>Equilibrium actions</em> refer to a set of strategies where no player has an incentive to change their strategy, given the strategies of the other players. In other words, at equilibrium, each player’s strategy is optimal, given the strategies of the other players. In network games, the equilibrium actions depend on the graph structure, along with other parameters dependent on the game.</p> <p>Consider, for example, a scenario where individuals on a social network can decide how much time to spend on the platform. In such a case, their behaviours may be influenced by their friends on the network, which creates a strategic interdependence between players. For instance, if Joe’s friends spend a lot of time on the platform, Joe might perceive a greater benefit from using the platform himself.</p> <p>In a different setting, Joe is a user of an e-commerce platform deciding whether to buy a book. If his friend has already purchased the book, Joe might be less likely to buy it, as he can borrow it from his friend. These examples illustrate how actions in network games can be affected by the actions of neighbouring players, leading to strategic interdependence and the emergence of equilibrium actions.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/network_games_examples-480.webp 480w,/assets/img/blog/network_games/network_games_examples-800.webp 800w,/assets/img/blog/network_games/network_games_examples-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/network_games_examples.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Examples of network games. Left: a person is likely to spend more time on a social app if their friends are also spending time on the app. Right: a person has less incentive to buy a book if their friend bought it, because they can borrow it.</figcaption> </figure> <h2 id="inferring-the-network-from-the-actions">Inferring the Network from the Actions</h2> <p>In the above examples, we assumed to know the friends of Joe, i.e., the network structure of the game. However, in many situations, the underlying network structure is not directly available to us. Instead, we may only observe the equilibrium actions that result from the interactions between agents. In these cases, a crucial question is whether we can reconstruct the network structure based solely on these equilibrium actions. Knowing the network structure can be helpful in predicting behaviour and planning network-based interventions, such as marketing campaigns or information diffusion.</p> <p>It was previously shown that, under specific assumptions about the mathematical form of the utility function and game types, it is possible to reconstruct the graph governing the network game<d-cite key="leng2020learning"></d-cite>. However, such assumptions can be unrealistic, especially when little is known about the game being played. To address this, in a recent paper<d-cite key="rossi2022networkgames"></d-cite> we developed an approach that does not require assumptions about the form of the utility function and can be applied to a broad range of network games.</p> <p>We start by studying three common types of network games, <em>Linear Quadratic</em>, <em>Linear Influence</em>, and <em>Barik-Honorio</em><d-cite key="barik2020provable"></d-cite>. The three types of games differ by the form of the utility function, leading to different levels of smoothness of the actions in the graph.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/network_games_types-480.webp 480w,/assets/img/blog/network_games/network_games_types-800.webp 800w,/assets/img/blog/network_games/network_games_types-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/network_games_types.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Specific instances of three different types of games common in the game theory literature. The colours represent the actions taken by the players, which in this case are continuous values normalised between -1 and 1.</figcaption> </figure> <p>For example, <em>Linear Quadratic</em> games have the following utility function:</p> \[u_i = b_i x_i -\frac{1}{2} x_i^2 + \beta \sum_{j \in \mathcal{N}_i} a_{ij} x_i x_j\] <p>where \(x_i\) is the continuous action taken by player $i$, \(b_i\) is the player’s <em>marginal benefit</em>, \(\beta\) is a game parameter representing the strength of dependencies between actions of neighbours in the network, and \(a_{ij}\) is the entry in the adjacency matrix of the graph representing the strength of the connection between \(i\) and \(j\). The first term represents the marginal benefit for taking a larger action, the second (quadratic) term represents the cost for taking the action, while the third term represents the relation with the neighbours actions<d-footnote>If $\beta$ is positive, the incentive of a player to take a higher action is increasing in the number of their neighbours also taking a higher action, something referred to as a <i>strategic complement relationship</i>. On the other hand, if $\beta$ is negative the incentive of a player to take a higher action is decreasing in the number of their neighbours taking a higher action (<i>strategic substitute relationship</i>).</d-footnote>. Taking as example the aforementioned scenario of time spent on a social platform, the first term of the equation would capture the individual benefit from using the platform, such as staying up-to-date with the news, the second term would represent the cost from doing so, such as having less time to do other more important things, while the third term would capture the interdependence with the friends actions. In particular, \(\beta\) would be positive, as a person has an incentive to spend more time on the app if their friends (neighbours) do so.</p> <p>The pure-strategy <a href="https://en.wikipedia.org/wiki/Nash_equilibrium">Nash equilibrium</a> of <em>Linear Quadratic</em> games is:</p> \[\mathbf{x}^* = \big( \mathbf{I} - \beta \mathbf{A} \big)^{-1} \mathbf{b}\] <p>where \(\mathbf{x}\)* is a vector of dimension \(n\) (equal to the number of players, or nodes of the graph), \(\mathbf{A}\) is the unknown \(n \times n\) adjacency matrix of the graph, \(\mathbf{b}\) is the \(n\)-dimensional vector of marginal benefits of the players.</p> <p>Similar formulas can be derived for <em>Linear Influence</em> and <em>Barik-Honorio</em> games. A formula for the equilibrium actions \(\mathbf{x}^*\) that generalizes all three games has the form<d-footnote>In this formula, the choice $f(\mathbf{A})=(\mathbf{I} - \beta\mathbf{A})^{-1}$ and $h(\mathbf{b})=\mathbf{b}$ yields a Linear Quadratic game, $f(\mathbf{A})=\mathbf{A}^{-1}$ and $h(\mathbf{b})=\mathbf{b}$ a Linear Influence game, and $f(\mathbf{A})=\mathbf{u}_1$ (the largest eigenvector of $\mathbf{A}$) and $h(\mathbf{b})=1$ a game of the Barik-Honorio type. </d-footnote></p> \[\mathbf{x}^* = \mathcal{F} (\mathbf{A}) \mathcal{H} (\mathbf{b})\] <p>where the function \(\mathcal{F} (\mathbf{A})\) accounts for the influence from the actions of one’s neighbours in the network and encodes the specific utility function of the game, and conversely, \(\mathcal{H} (\mathbf{b})\) is only affected by one’s characteristics, such as the marginal benefit of an individual player.</p> <p>In the paper we further show<d-footnote>Section 3.3 in our paper<d-cite key="rossi2022networkgames"></d-cite>.</d-footnote> that the players’ actions contain information about the spectrum of the graph, confirming that it is possible to reconstruct the graph structure from only the actions and justifying our approach outlined below.</p> <h2 id="machine-learning-approach">Machine Learning Approach</h2> <p>To tackle the problem of inferring the network structure of a game, we approach it as a machine learning problem. We train a model to map the players’ actions to the network structure of the game, without any prior knowledge of the underlying utility function. To achieve this, we gather a dataset of actions and network pairs (\(\mathbf{x}\), \(\mathbf{A}\)) from games played with the same utility function (although this function is unknown to us). This allows us to avoid making strong assumptions about the utility function and instead train a model that is agnostic to it.</p> <p>Such an approach is particularly useful in scenarios where social network and decision data exist for a small population, and we aim to learn the mapping from decisions to the network structure of a larger population. For instance, governments, public agencies, and researchers can collect social network data on a small population by asking individuals to nominate their friends, and then use the proposed method to learn the network interactions for a larger population in a cost-effective manner.</p> <p>Our ML model has an encoder-decoder architecture that is invariant to the permutation of both the players and the games, corresponding to the rows and columns of the \(n \times K\) matrix \(\mathbf{x}\), where \(k\) denotes the number of games. To achieve this, we modify a Transformer model, which is naturally permutation-invariant over the set of nodes but not over the set of games.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/model.svg" sizes="95vw"/> <img src="/assets/img/blog/network_games/model.svg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Diagram representing the encoder-decoder architecture of our model. The $n \\times K$ input matrix $\\mathbf{x}$ containing the players' actions is encoded into the $n \\times F \\times K$ tensor $\\mathbf{Z}$, where $\\mathbf{z}_{ik}$ is the embedding for node $i$ in game $k$, obtained by attending over the actions of the other players in the same game. $\\mathbf{Z}$ is then decoded into the $n \\times n$ adjacency matrix $\\tilde{\\mathbf{A}}$ where the entry $\\tilde{\\mathbf{a}}_{ij}$ contains the probability of an edge between $i$ and $j$.</figcaption> </figure> <p>Our encoder produces \(K\) vectors for each player as follows:</p> <p>The scalar action \(\mathbf{x}_{ik}\) of player \(i\) for game \(k\) is first passed through a non-linear transformation resulting in an \(F\)-dimensional vector</p> \[\mathbf{y}_{ik} = \text{ReLU}(\mathbf{x}_{ik}\mathbf{W} + \mathbf{b})\] <p>We then calculate the unnormalised attention scores</p> \[s_{ij} = \sum_{k} \mathbf{y}_{ik}^T \mathbf{W} \mathbf{W}_k \mathbf{y}_{jk}\] <p>between players \(i\) and \(j\) by first computing per-game scores using a ‘learned dot-product’ with query and key weight matrices \(\mathbf{W}\) and \(\mathbf{W}_k\) as in the original Transformer<d-footnote>See "<a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>" blog post for an intuitive explanation of the Transformer and the role of the query and weight matrices.</d-footnote>, and then summing them over the games.</p> <p>The attention scores</p> \[\alpha_{ij} = \text{softmax}_{j}(u_{ij})\] <p>are obtained by taking the softmax over the unnormalised scores over the second dimension.</p> <p>Finally, the \(F\)-dimension embedding</p> \[\mathbf{z}_{ik} = \phi\left(\sum_{v} \alpha_{ij}\mathbf{y}_{jk}\right)\] <p>of node \(i\) for game \(k\) is obtained by aggregating the \(\mathbf{y}_{ik}\) vectors of other nodes weighted by the attention scores, before passing the result through a 2-layer MLP \(\phi\).</p> <p>The decoder outputs probabilities for each entry of the adjacency matrix by aggregating the \(k\) vectors for players \(i\) and \(j\). This is done by taking the dot product of the two vectors for each game and summing the results before feeding them into a multilayer perceptron (MLP):</p> \[\hat{a}_{ij} = \psi\left(\sum_{k} \mathbf{z}_{ik} \odot \mathbf{Z}_{jk}\right)\] <p>where \(\odot\) represents the dot product and \(\psi\) is a 2-layer MLP.</p> <p>The resulting model is permutation-invariant over the set of games. This means that it produces the same predicted adjacency matrix regardless of the order in which the games are presented. To achieve this, the model computes separate node embeddings for each game and aggregates them through summation, which is a permutation-invariant operation. In practice, this property of the model ensures that we obtain consistent and reliable predictions, regardless of how the input data is ordered.</p> <h2 id="experimental-results">Experimental Results</h2> <p>We conducted experiments to validate the effectiveness of our approach in learning the network structure from players’ actions, using both synthetic and real-world datasets. As baselines, we used DeepGraph<d-cite key="belilovsky2017learning"></d-cite> (the only machine learning approach we are aware of), optimisation methods specific to the game type, and simple correlation and anticorrelation of actions between nodes.</p> <p>On synthetic datasets, our model, NuGgeT, consistently outperformed previous methods across a range of different games and graphs types.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/linear_quadratic_results-480.webp 480w,/assets/img/blog/network_games/linear_quadratic_results-800.webp 800w,/assets/img/blog/network_games/linear_quadratic_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/linear_quadratic_results.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">We report the results on <em>Linear Influence</em> games (see the paper for <em>Linear Quadratic</em> and <em>Barik-Honorio</em>) on three different types of synthetic graphs (Watts–Strogatz, Erdős–Rényi and Barabási–Albert) and with varying smoothness of the marginal benefit (a hyperparameter of this type of game). Our method, called NuGgeT, consistently outperforms the baselines.</figcaption> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/ablation-480.webp 480w,/assets/img/blog/network_games/ablation-800.webp 800w,/assets/img/blog/network_games/ablation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/ablation.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The performance of our model in learning the mapping depends on the number of available games and training graphs, and we conducted ablations to evaluate both factors. Generally, a higher number of games and graphs are beneficial for our approach. However, we observe that the model performance tends to plateau at around 100 games and 500 graphs in most cases.</figcaption> </figure> <p>We further validated our approach on two real-world datasets: the <em>Indian Villages</em> dataset<d-footnote>The dataset accompanies the paper <a href="https://www.science.org/doi/10.1126/science.1236498">"The Diffusion of Microfinance"</a> <d-cite key="banerjee2013diffusion"></d-cite>. Two authors of the paper (Abhijit Banerjee and Esther Duflo) went on to receive the 2019 Economics Nobel prize.</d-footnote> and the <em>Yelp Ratings</em> dataset<d-footnote><a href="https://business.yelp.com/data/resources/open-dataset/">Yelp Open dataset</a>.</d-footnote>. The former contains data from a survey of social networks in 75 villages in India. Each village constitutes a social network graph, where nodes are households and edges are self-reported friendships. We consider as actions the number of rooms, number of beds and other decisions families have to make related to their household. The reasoning is that if neighbours adopt a specific facility, villagers tend to gain higher payoff by doing the same, i.e., complying with social norms.</p> <p>The <em>Yelp Ratings</em> dataset consists of user ratings of businesses and social connectivity between users. We extracted 5000 sub-graphs representing communities from the raw data, where the actions were the average rating of users for 22 categories of businesses.</p> <p>On both real-world datasets, NuGgeT outperforms previous methods, showcasing the efficacy of our approach in cases where the game utility is not explicitly known. The gain is particularly large on the <em>Indian Villages</em> dataset, where the competing DeepGraph method fails to learn altogether. We conjecture this is due to NuGgeT being more data-efficient thanks to its built-in invariances, as confirmed by the above ablation over the number of training graphs.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/network_games/table_results-480.webp 480w,/assets/img/blog/network_games/table_results-800.webp 800w,/assets/img/blog/network_games/table_results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/blog/network_games/table_results.png" class="img-fluid" width="500px" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">The table reports ROC AUC on the test set. NuGgeT outperforms previous methods on both the two real-world datasets we tested on, confirming its efficacy in cases where the game utility function is not explicitly known.</figcaption> </figure> <p>In conclusion, our paper highlights the fruitful connection between game theory and graph machine learning, particularly in the context of network games. By developing a new machine learning approach to infer network structure from observed game outcomes, we show the potential for utilising game theory ideas to enhance machine learning and vice versa. Looking forward, there is ample opportunity to explore further connections between network games and graph neural networks, paving the way for more exciting developments in these fields.</p>]]></content><author><name>Emanuele Rossi</name></author><summary type="html"><![CDATA[Network games are a powerful tool for modelling strategic interactions between individuals or organisations played out on networks, where a player's payoff depends not only on their own actions but also on those of their neighbours. Such games have numerous applications in economics and social sciences, including studying the spread of influence in social networks, the dynamics of financial markets, and the formation of alliances in international relations. The study of network games typically assumes the underlying network structure to be known, which is often wishful thinking. Recently, machine learning approaches have been proposed to tackle this problem by leveraging the observed actions of players to learn the underlying network structure. In this blog post, we outline a novel approach that uses a transformer-like architecture to infer the network structure of a game without explicit knowledge of the utility function associated with the game.]]></summary></entry><entry><title type="html">Temporal Graph Learning in 2023 | Towards Data Science</title><link href="https://emalgorithm.github.io/blog/2023/temporal-graph-learning-in-2023-towards-data-science/" rel="alternate" type="text/html" title="Temporal Graph Learning in 2023 | Towards Data Science"/><published>2023-01-16T00:00:00+00:00</published><updated>2023-01-16T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2023/temporal-graph-learning-in-2023--towards-data-science</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2023/temporal-graph-learning-in-2023-towards-data-science/"><![CDATA[<p>Publish AI, ML &amp; data-science insights to a global community of data professionals. The story so far Real world networks such as social, traffic and citation networks often evolve over time and the field of Temporal Graph Learning (TGL) aims to extract, learn and predict from these evolving networks. Recently, TGL has gained increasing attention from the ML community, with a surge in the number of papers and the first workshop in this area held last year at NeurIPS 2022!This post was co-authored with Emanuele Rossi, Michael Galkin and <em>Kellin Pelrine</em>. Thanks to Farimah Poursafaei for the helpful feedback.In this blog post, we present major progress in TGL until 2022 and discuss promising future directions. Note that we use the term “dynamic graph” and “temporal graph” interchangeably. If you want to learn or start a project in TGL, this article would be a good reference and starting point.Please share with us in the comment section any other advances you are excited about.Table of Contents:In this section, we provide a brief overview of some well-known TGL methods in the literature. There exist two main broad classes of methods for learning on Continuous-Time Dynamic Graphs (CTDGs): Temporal Graph Networks and Walk Aggregating methods. For more details on the formulation of CTDGs, see this survey by Kazemi et al.Temporal Graph Networks (TGNs) generalize Message Passing Neural Networks (MPNNs) to temporal graphs. They do so by introducing a node memory which represents the state of the node at a given time, acting as a compressed representation of the node’s past interactions. Every time two nodes are involved in an interaction, they send messages to each other which are then used to update their memories. When computing the embedding of a node, an additional graph aggregation is performed over the temporal neighbors of the node, using both the original node features and memory at that point in time. Below we show a diagram of the computation of TGN.Computations of TGN on a batch of training edges. Image source: Rossi et al.TGNs are a general framework that generalizes previous models such as Joint Dynamic User-Item Embeddings (JODIE) and Temporal Graph Attention (TGAT) as specific cases. A more comprehensive introduction to TGNs can be found in the below blog post by one of the authors.Temporal Graph NetworksWalk Aggregating methods such as Causal Anonymous Walks (CAW) instead rely on (temporal) random walks. In particular, to predict the existence of a link (u, v) at time t, CAW first extracts multiple random walks starting from u and v such that the timestamps of edges in a walk can only be monotonically decreasing. The walks are first anonymized by replacing each node identifier with a count vector of how many times that node appears at each of the possible positions in the walks. Each walk is then encoded using an RNN, and the encodings are then aggregated by using self-attention or taking a simple average.There is a large body of work studying the expressive power of GNNs operating on static graphs. Xu et al. 2019 first characterized the discriminative power of Graph Neural Networks (GNNs) by connecting them to the Weisfeiler-Lehman (WL) graph isomorphism test and showing that many GNNs are no more powerful than the 1-WL test. Subsequent more expressive models such as subgraph GNNs, graph transformers and higher order GNNs are designed to be more expressive than 1-WL test (below is link to Michael Bronstein‘s excellent blog post on how to go beyond WL test).Graph Neural Networks beyond Weisfeiler-Lehman and vanilla Message PassingUntil this year, there has been little work on understanding the expressive power of TGL methods. The first effort to bridge this gap was by Ribeiro et al. where the key idea is to categorize existing TGL methods into time-and-graph and time-then-graph frameworks.Converting a TG into time-then-graph representation. image source: Ribeiro et al.1️). In time-and-graph, GNNs are used to generate node embeddings on the snapshot graph at each time thus forming a sequence of node embeddings.2️). In time-then-graph, each edge in the TG is converted to a time series which indicates at which time the edge exists, therefore collapsing the temporal edges into edge features in a static graph.It was shown that a time-then-graph representation can be constructed from any given time-and-graph representation thus proving time-then-graph is at least as expressive as time-and-graph. With the static representation in time-then-graph, we can directly use the WL-test expressiveness framework from the static graph for TGL methods. In this way, time-then-graph is more expressive than time-and-graph as long as a 1-WL GNN is used as the backbone model.Souza et al. also aims to establish the 1-WL expressiveness framework for TGL methods. Notably, they view a CTDG as a sequence of time-stamped multi-graphs where the multi-graph G(t) at a given time t is obtained by sequentially applying all events prior to t. A multi-graph here means there can be multiple edges between two nodes in the graph and the edge attribute is the timestamp information.Now, the Temporal WL test can be defined by applying the WL test on the multigraphs constructed from CTDGs. Therefore, more expressive TGN methods must be injective on its temporal neighborhood (i.e. hashing two different multi-set nodes into different colors), called injective MP-TGNs. Souza et al. also analyzed walk based TGNs such as CAW and show that MP-TGNs and CAW are not more expressive than each other (as seen above). Their proposed PINT method combines benefits from both categories of methods thus being the most expressive. The example below shows two temporal graphs that MP-TGNs are unable to distinguish. The colors are node labels and the edge has timestamps starting from t₁.Examples of temporal graphs for which MP-TGNs are unable to distinguish graph structures such as diameter, girth, and number of cycles. Image source: Souza et al.To a large extent, the evaluation procedure in TGL is relatively under-explored and heavily influenced by static graph learning. For example, evaluation on the link prediction task on dynamic graphs (or dynamic link prediction) often involves: 1). fixed train, test split, 2). random negative edge sampling and 3). small datasets from similar domains. Such evaluation protocols often lead to result tables where reported metrics are already around 95+% and it’s really hard to distinguish whether new models bring any benefit or just rehash existing methods again.A typical temporal link prediction result table reporting Average Precision (AP). Are we really making any progress when even baselines yield 98%? Image source: Souza et al.You et al. discussed the limitations of current TGL methods in model design, evaluation settings and training strategies for Discrete Time Dynamic Graphs (DTDGs). They argue that the evolving nature of data and models are not accounted for. In standard evaluation, all time points are split chronologically into a training set, an evaluation set and then a test set. The split is fixed for a given dataset.They pointed out that such a fixed split means only edges from the chosen test period would be evaluated thus long term behavior potentially spanning training, validation and test period would not be correctly evaluated. In addition, many TGL methods are stale at test time, meaning that the model representation is not updated with information during evaluation. Consider an example transaction graph, if information on the prior day is available, the user would likely want to update the model with such information to achieve the best possible performance. Therefore, a live-update evaluation is proposed where models are finetuned with newly observed data, utilizing historical information and predicting future links.Grey / red bars indicate the amount of recurring / novel edges respectively in the Wikipedia / MOOC dataset. Many edges recur over time in temporal graphs. Image source: Poursafaei et al.Recent work by two of the authors examines which negative edges to sample for evaluation of CTDG methods and introduces more datasets from diverse domains. When evaluating dynamic link prediction, negative edges are often randomly sampled from any node pair. However, many edges in a temporal graph recur over time (as seen in the figure above). Considering the sparsity of real world graphs, the majority of node pairs are unlikely to form an edge. Therefore, random negative edges can be seen as easy negative edges.Avg. Performance of TGL methods. Using more difficult negative edges significantly impacts model performance. The simple baseline EdgeBank also works surprisingly well. Image source: Poursafaei et al.Now, what can be considered as hard negative edges? First, we introduce the historical negative edges which are edges that appeared in the training set but are absent in the current test step. We also define inductive negative edges as test edges which occurred previously in the test set but are not present at the current step. Lastly, we propose a baseline EdgeBank relying solely on memorizing past edges (essentially a hashtable of seen edges). In the plot above, we see that by changing the negative edges for evaluation, the average performance of existing TGL methods reduces significantly in the historical and inductive setting when compared to the standard setting. EdgeBank is also a surprisingly strong baseline for the standard setting. For details, see the blog below from one of the authors.Towards Better Link Prediction in Dynamic GraphsIn the realm of Knowledge Graphs (KGs), temporal setup is slightly different from the homogeneous world, i.e., timestamped graph snapshots are not that common. Instead, some (or all) triples have an accompanying (start time, end time) pair of attributes denoting the time frame when a given fact was true. Triples thus become quintuples, or, in Wikidata, time attributes become qualifiers of a more general statement (main triple + multiple key-value qualifiers), and statements form so-called hyper-relational KGs.For example, (President of the French republic, office holder, Nicolas Sarkozy, 2007, 2012) is a quintuple describing the time period where Nicolas Sarkozy was the President of the French republic. Alternatively, there can be only one timestamp per triple (forming quadruples). The most common prediction task is scoring head/tail prediction given the time attributes, e.g. , (President of the French Republic, office holder, <strong>???</strong>, 2007, 2012) – this can be considered as a particular case of the hyper-relational link prediction where qualifiers are only dateTime literals. A classic example of temporal KG completion model is TNTComplex (ICLR 2020).Krause et al. has taken the first effort towards bridging the gap between temporal KGs and homogeneous graphs. In this work, the authors propose a framework to formalize various temporal aspects in KGs. Namely, they define temporal KGs as local extensions, i.e., graphs that have timestamps on the edges, and dynamic KGs as global extensions, i.e., graphs that change topology over time by adding or removing nodes and edges. Even more, there can exits combinations of those basic types, e.g., a temporal and dynamic KG would be called incremental. We hope this work would bring a bit more order and clarity to the hectic literature on temporal KGs and the community would stick to the nice taxonomy. Next step: finalize a proper evaluation protocol for those graph types.Temporal and Dynamic KGs (and their combinations). Image source: Krause et al.Wang et al. addressed the task of few-shot link prediction over temporal + dynamic graphs where the edges have timestamps and new nodes might appear at later timesteps (Incremental graphs as to the above classification by Krause et al.). The few-shot scenario makes the task even more challenging – we only have access to a limited number of training and inference points (usually, &lt;5) to reason about the queries link. Here, the authors propose MetaTKGR, a meta-learning based approach that builds representations of new nodes by aggregating features of existing nodes within a certain delta t temporal neighborhood. The scalar difference between timestamps is vectorized via the Fourier transform.Components of MetaTKGR. Image source: Wang et al.The lack of large datasets and challenging tasks has been holding back research on Temporal Graph Learning in the past few years. Luckily, new datasets from diverse domains are emerging. For example, Poursafaei et al. introduced six new publicly available TG datasets from the transportation, politics, economics and proximity domains. However, the field is still lacking a consistent effort to standardize benchmarks and evaluation to a high quality, what OGB did for static graphs. We hope that in 2023, we can see more standardized TG benchmarks with a focus on real applications.Regarding libraries, a well-known one is <a href="https://pytorch-geometric.readthedocs.io/en/latest/">Pytorch Geometric</a> Temporal, an extension of Pytorch Geometric for temporal graphs. However, Pytorch-Geometric Temporal seems to only feature discrete-time methods and datasets. A library which also includes continuous-time methods would be a great added value for the community. Recently, Zhou et al. introduced TGL, a unified framework for large-scale offline Temporal Graph Neural Network training. In particular, on a 4-GPU machine, TGL can train one epoch of more than one billion temporal edges within 1–10 hours.We list links to various TGL libraries and datasets below.In the recent COVID-19 pandemic, epidemic modeling is instrumental for understanding the spread of the disease as well as designing corresponding intervention strategies. Human contact networks are in fact temporal graphs. By combining contact graphs with classical compartment based models such as SEIR and SIR, we can more accurately forecast COVID-19 infection curve and go beyond the homogenous mixing assumption (all individuals are equally likely to be in contact with each other).Chang et al. derived a temporal mobility network from cell phone data and mapped the hourly movement of 98 million people from census block groups (CBGs) to specific points of interest (POIs) in the US. By combining the hourly contact network with SEIR models on the CBG level, they are able to accurately fit real infection trajectory. In particular, the model shows that some ‘superspreader’ POIs such as restaurants and fitness centers account for a large majority of the infections. Also, differences in mobility between racial and socioeconomic groups lead to different infection rates among these groups. This work showcased the real world potential of utilizing large scale temporal graphs for disease forecasting and informing policies on intervention strategies.Besides human contact networks, dynamic transportation networks also play an important role in the spread of COVID-19. In recent work by one of the authors, we incorporated daily flight networks into the SEIR model to estimate imported COVID-19 cases. By incorporating flight networks, it is possible for early detection of outbreaks and forecast the impact of travel restrictions. See the blog post by one of the authors for more details.Despite the empirical success of temporal-graph-based disease models, it is also important to answer questions such as “how does the contact network structure impact the spread of disease?” and “what is the best way to modify the contact pattern such that the spread of COVID-19 can be slowed or prevented?” Holme et al. compared the difference in outbreak characteristics between using temporal, static and fully-connected networks on eight network datasets and examined various network structures affecting the spread of the disease. They showed that converting temporal networks into static ones can lead to severe under- or over-estimation of both the outbreak size and extinction time of the disease.What are the next steps for TGL on epidemic modeling?1️). First, forecasting the entire contact or mobility network snapshot for the immediate future is a crucial challenge. With the predicted structure, we can apply network based SEIR models to estimate the infection curve.2️). Second, defining and understanding the impact of interaction patterns on the contact network is crucial for policy making and interpretability. Analyzing the interplay between graph structures and the infection curve can help us identify the most effective intervention strategies.Anomaly detection is a fundamental task in analyzing temporal graphs which identifies entities that deviate significantly from the rest. For example, fraud detection can be modeled as detecting abnormal edges in a transaction network and traffic accident identification can be seen as detecting anomalous events in a traffic network.There is growing interest in utilizing the representation power of temporal graph networks for anomaly detection. <a href="https://arxiv.org/abs/2005.07427">Cai et al.</a> designed an end-to-end structural temporal Graph Neural Network model for detecting anomalous edges, called StrGNN. An enclosing subgraph, a k-hop subgraph centered around an edge, is first extracted based on the edge of interest to reduce computational complexity. A Graph Convolutional Neural Network (GCN) is then used to generate structural embedding from the subgraph. Gated Recurrent Units (GRUs) are then used to capture temporal information. One of the challenges of anomaly detection is the lack of labeled examples. Therefore, Cai et al. proposed to generate “context-dependent” negative edges by replacing one of the nodes in a normal edge and training the model with these negative edges.When comparing with unsupervised, non-GNN based anomaly detection methods such as SEDANSPOT and AnomRank, GNN based methods can easily incorporate any given attribute and has the potential to achieve stronger performance. However, there are two significant challenges for GNN based approaches.1). First, how to scale to dynamic graphs with millions of edges and nodes? This is an open question for both the GNN module in extracting the graph features but also the temporal module such as GRUs and transformers in processing long term information.2️). Second, how to produce accurate explanations for the detected anomalies? In real applications, detected anomalies are often verified and then potentially resulting in punitive measures for those detected entities. GNN explainability on dynamic graphs remains an open challenge.LAD detects 2013 as a change point in the Canadian MP voting network due to abnormal amounts of edges between political parties. Image source: Huang et al.The task of change point detection aims to detect time points in a dynamic graph where the graph structure or distribution deviates significantly from what was observed before. This change can be attributed to external events (such as traffic disruption and COVID-19 related flight restrictions) or simply natural evolution of the dynamic graph. Recent work by one of the authors utilized the eigenvalues of the Laplacian matrix of each graph snapshot to embed the graph structure while applying sliding windows to compare the changes in graph structure in the long and short term. In the above, the proposed Laplacian Anomaly Detection (LAD) method detects a change in the Canadian Member of Parliament (MP) voting network due to increased edges between political parties. This coincides with Justin Trudeau being selected as the Liberal party leader in 2013.Misinformation spreads in different patterns and rates when compared to true information (Vosoughi et al.). There has been considerable research studying these network patterns in a static graph while dynamic graph based methods are underexplored (Song et al.). However, in the past year an increased amount of TGL methods were employed for misinformation detection and understanding. For instance, Zhang et al. developed a method based on Temporal Point Processes while Dynamic GCN (DynGCN) and DGNF are dynamic GNN based methods.The illustration below shows the architecture of DynGCN. They construct graph snapshots with even spacing in time, feed each through GCN layers, and then combine the representations and learn the snapshots’ evolution patterns using attention. This is a relatively simpler approach to leverage temporal information compared to some methods discussed above like TGN or CAW, but nonetheless gives better performance than previous state-of-the-art for misinformation detection on the datasets that the authors examined.DynGCN processes individual graph snapshots using GCN layers with shared weights, then combines the representations over time with an attention mechanism. Image source: Choi et al.Dynamic interaction patterns are shown to be quite informative for misinformation detection (Plepi et al.). With significant recent advances in TGL methods, we can expect novel state-of-the-art misinformation detection methods that incorporate dynamic graphs.2022 has seen increased attention in TGL from the ML community. The first ever TGL workshop was held at NeurIPS 2022. The recordings of the talks and panel will be available soon on the NeurIPS virtual site. The accepted papers are available on the workshop website. Keep an eye out for announcements of new iterations of the TGL workshop there and join the workshop slack (up-to-date link in the website) to engage with the community. This year, we are also planning a TGL reading group, if you would like to share your work or be involved in co-organizing the reading group, please email [email protected]Written ByShare This ArticleTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.Step-by-step code guide to building a Convolutional Neural Network A beginner’s guide to forecast reconciliation Here’s how to use Autoencoders to detect signals with anomalies in a few lines of… Feature engineering, structuring unstructured data, and lead scoring Solving the resource constrained project scheduling problem (RCPSP) with D-Wave’s hybrid constrained quadratic model (CQM) An illustrated guide on essential machine learning concepts Derivation and practical examples of this powerful concept Your home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The story so far]]></summary></entry><entry><title type="html">Accelerating and scaling temporal graph networks on the Graphcore IPU</title><link href="https://emalgorithm.github.io/blog/2022/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu/" rel="alternate" type="text/html" title="Accelerating and scaling temporal graph networks on the Graphcore IPU"/><published>2022-06-14T00:00:00+00:00</published><updated>2022-06-14T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2022/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2022/accelerating-and-scaling-temporal-graph-networks-on-the-graphcore-ipu/"><![CDATA[<p>Jun 14, 2022 Written By:Prof. Michael Bronstein, Emanuele Rossi, Daniel JustusPopular TopicsWe’re HiringJoin us and build the next generation AI stack - including silicon, hardware and software - the worldwide standard for AI computeMichael Bronstein is the DeepMind Professor of AI at the University of Oxford and Head of Graph Learning Research at Twitter. A version of this post first appeared on his blog. Professor Bronstein co-authored this post with Emanuele Rossi from Twitter and Daniel Justus from Graphcore.UPDATE: TGN Training is now available to run as a Paperspace Gradient Notebook.Graph-structured data arise in many problems dealing with complex systems of interacting entities. In recent years, methods applying machine learning methods to graph-structured data, in particular Graph Neural Networks (GNNs), have witnessed a vast growth in popularity.The majority of GNN architectures assume that the graph is fixed. However, this assumption is often too simplistic: since in many applications the underlying system is dynamic, the graph changes over time. This is for example the situation in social networks or recommendation systems, where the graphs describing user interaction with content can change in real-time. Several GNN architectures capable of dealing with dynamic graphs have recently been developed, including our own Temporal Graph Networks (TGNs)[1].A graph of interactions between people is changing dynamically by gaining new edges at timestamps t1 and t2In this post, we explore the application of TGNs to dynamic graphs of different sizes and study the computational complexities of this class of models. We use Graphcore’s Bow Intelligence Processing Unit (IPU) to train TGNs and demonstrate why the IPU’s architecture is well-suited to address these complexities, leading to up to an order of magnitude speedup when comparing a single IPU processor to an NVIDIA A100 GPU.The TGN architecture, described in detail in our previous post, consists of two major components: First, node embeddings are generated via a classical graph neural network architecture, here implemented as a single layer graph attention network[2]. Additionally, TGN keeps a memory summarizing all past interactions of each node. This storage is accessed by sparse read/write operations and updated with new interactions using a Gated Recurrent Network (GRU)[3].TGN model architecture. Bottom row represents a GNN with a single message passing step. Top row illustrates the additional memory for each node of the graphWe focus on graphs that change over time by gaining new edges. In this case, the memory for a given node contains information on all edges that target this node as well as their respective destination nodes. Through indirect contributions, the memory of a given node can also hold information about nodes further away, thus making additional layers in the graph attention network dispensable.We first experiment with TGN on the JODIE Wikipedia dataset[4], a bipartite graph of Wikipedia articles and users, where each edge between a user and an article represents an edit of the article by the user. The graph consists of 9,227 nodes (8,227 users and 1,000 articles) and 157,474 time-stamped edges annotated with 172-dimensional LIWC feature vectors[5] describing the edit.During training, edges are inserted batch-by-batch into the initially disconnected set of nodes, while the model is trained using a contrastive loss of true edges and randomly sampled negative edges. Validation results are reported as the probability of identifying the true edge over a randomly sampled negative edge.Intuitively, a large batch size has detrimental consequences for training as well as inference: The node memory and the graph connectivity are both only updated after a full batch is processed. Therefore, the later events within one batch might rely on outdated information as they are not aware of earlier events in the batch. Indeed, we observe an adverse effect of large batch sizes on task performance, as shown in the following figure:Accuracy of TGN on JODIE/Wikipedia data when training with different batch sizes and validating with a fixed batch size of 10 (left) and when training with a fixed batch size of 10 and validating with different batch sizes (right).However, the use of small batches emphasizes the importance of fast memory access for achieving a high throughput during training and inference. The IPU with its large In-Processor-Memory, therefore, demonstrates an increasing throughput advantage over GPUs with smaller batch size, as shown in the following figure. In particular, when using a batch size of 10 TGN can be trained on the IPU about 11 times faster, and even with a large batch size of 200 training is still about 3 times faster on the IPU.Throughput improvement for different batch sizes when using a single IPU out of a Bow2000 IPU system compared to an NVIDIA A100 GPU.To better understand the improved throughput of TGN training on Graphcore’s IPU, we investigate the time spent by the different hardware platforms on the key operations of TGN. We find that the time spent on GPU is dominated by the Attention module and the GRU, two operations that are performed more efficiently on the IPU. Moreover, throughout all operations, the IPU handles small batch sizes much more efficiently.In particular, we observe that the IPU’s advantage grows with smaller and more fragmented memory operations. More generally, we conclude that the IPU architecture shows a significant advantage over GPUs when the compute and memory access are very heterogeneous.Time comparison for the key operations of TGN on IPU and GPU with different batch sizes.While the TGN model in its default configuration is relatively lightweight with about 260,000 parameters, when applying the model to large graphs most of the IPU In-Processor-Memory is used by the node memory. However, since it is sparsely accessed, this tensor can be moved to off-chip memory, in which case the In-Processor-Memory utilisation is independent of the size of the graph.To test the TGN architecture on large graphs, we apply it to an anonymized graph containing 261 million follows between 15.5 million Twitter users[6]. The edges are assigned 728 different time stamps which respect date ordering but do not provide any information about the actual date when follows occurred. Since no node or edge features are present in this dataset, the model relies entirely on the graph topology and temporal evolution to predict new links.Since the large amount of data makes the task of identifying a positive edge when compared to a single negative sample too simple, we use the Mean Reciprocal Rank (MRR) of the true edge among 1000 randomly sampled negative edges as a validation metric. Moreover, we find that the model performance benefits from a larger hidden size when increasing the dataset size. For the given data we identify a latent size of 256 as the sweet spot between accuracy and throughput.Mean Reciprocal Rank among 1000 negative samples for different hidden sizes of the model.Using off-chip memory for the node memory reduces throughput by about a factor of two. However, using induced subgraphs of different sizes as well as a synthetic dataset with 10× the number of nodes of the Twitter graph and random connectivity we demonstrate that throughput is almost independent of the size of the graph (see table below). Using this technique on the IPU, TGN can be applied to almost arbitrary graph sizes, only limited by the amount of available host memory while retaining a very high throughput during training and inference.Time per batch of size 256 for training TGN with hidden size 256 on different graph sizes. Twitter-tiny is of a similar size as the JODIE/Wikipedia dataset.As we have repeatedly noted previously, the choice of hardware for implementing Graph ML models is a crucial, yet often overlooked problem. In the research community, in particular, the availability of cloud computing services abstracting out the underlying hardware leads to certain “laziness” in this regard. However, when it comes to implementing systems working on large-scale datasets with real-time latency requirements, hardware considerations cannot be taken easily anymore. We hope that our study will draw more attention to this important topic and pave the way for future more efficient algorithms and hardware architectures for Graph ML applications. [1] E. Rossi et al., Temporal graph networks for deep learning on dynamic graphs (2020) arXiv:2006.10637. See accompanying blog post.[2] P. Veličković, et al., Graph attention networks (2018) ICLR.[3] K. Cho et al., On the properties of neural machine translation: Encoder-Decoder approaches (2014), arXiv:1409.1259.[4] S. Kumar et al., Predicting dynamic embedding trajectory in temporal interaction networks (2019) KDD.[5] J. W. Pennebaker et al., Linguistic inquiry and word count: LIWC 2001 (2001). Mahwah: Lawrence Erlbaum Associates 71.[6] A. El-Kishky et al., kNN-Embed: Locally smoothed embedding mixtures for multi-interest candidate retrieval (2022) arXiv:2205.06205.Share:Sign up for Graphcore updates:Feb 12, 2026 \ Careers, SiliconNov 10, 2025 \ Oct 09, 2025 \ Sign up below to get the latest news and updates:Why GraphcoreLegacy ProductsLegal</p>]]></content><author><name></name></author><summary type="html"><![CDATA[In this post, we explore the implementation of temporal GNNs on a new architecture developed by Graphcore that is tailored to graph-structured workloads.]]></summary></entry><entry><title type="html">Graph Machine Learning with Missing Node Features</title><link href="https://emalgorithm.github.io/blog/2022/graph-machine-learning-with-missing-node-features/" rel="alternate" type="text/html" title="Graph Machine Learning with Missing Node Features"/><published>2022-03-17T00:00:00+00:00</published><updated>2022-03-17T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2022/graph-machine-learning-with-missing-node-features</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2022/graph-machine-learning-with-missing-node-features/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Deep Learning on Dynamic Graphs</title><link href="https://emalgorithm.github.io/blog/2021/deep-learning-on-dynamic-graphs/" rel="alternate" type="text/html" title="Deep Learning on Dynamic Graphs"/><published>2021-01-25T00:00:00+00:00</published><updated>2021-01-25T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2021/deep-learning-on-dynamic-graphs</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2021/deep-learning-on-dynamic-graphs/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Simple Scalable Graph Neural Networks</title><link href="https://emalgorithm.github.io/blog/2020/simple-scalable-graph-neural-networks/" rel="alternate" type="text/html" title="Simple Scalable Graph Neural Networks"/><published>2020-08-08T00:00:00+00:00</published><updated>2020-08-08T00:00:00+00:00</updated><id>https://emalgorithm.github.io/blog/2020/simple-scalable-graph-neural-networks</id><content type="html" xml:base="https://emalgorithm.github.io/blog/2020/simple-scalable-graph-neural-networks/"><![CDATA[]]></content><author><name></name></author></entry></feed>